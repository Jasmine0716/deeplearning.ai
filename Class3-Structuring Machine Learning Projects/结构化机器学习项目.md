结构化机器学习项目

## 正交化
* 每一个影响/调整方面不相互重叠，如调整电视机的不同旋钮（调整宽度、高度等）
* early stopping和正交化：
    *  一般不用early stopping
    *  early stopping的理念和正交化相互矛盾，既影响训练集正确类又影响测试机

## 评估指标
* 查准率P
    * 分类器标记为1的对象中，有多少真的是1
* 查全率R
    * 对于所有标记是1的对象，分类器正确标记了多少
* 单一数值评估指标
    * 在存在多个评价方面时，为了方便整体比较，将多个方面进行融合
    * F1分数：查准率P和查全率R的调和平均数
     ![F1分数](http://www.ai-start.com/dl2017/images/02f4810d07913df9cf8b80e455c7d3aa.png)
    * 其他多指标：可以进行取平均值

* 满足和优化指标
    * 满足指标：设置值域，训练结果满足值域条件即可
    * 优化指标：非满足指标的其他指标，表现越优秀越好（<mark>一般只有一个</mark>）
    * 选择评判标准：在达到满足指标的训练中选择优化指标表现最好的训练方法和结果
    * eg：
        * 运行时间——满足指标  准确度——优化指标
        * 假阳性数量（智能设备被错误唤醒）——满足指标  正确唤醒——优化指标

* 评价指标的改进
    * 根据实际的情况进行改进：如不想看到正确率高的同时，却在错误中判入了色情图片
    * 为某些错误/关注的内容在计算错误率时增加权重
    ![增加权重](http://www.ai-start.com/dl2017/images/fabea6c50883bf1afaf630d068a4009f.png)

## 训练集/开发集/测试集的划分
* 基本概念：
    * 训练集：
    * 开发集：dev(development set)/交叉验证集，用搭配训练集测试训练效果，得到最满意的模型
    * 测试集：最终测试，完成系统开发之后，测试集可以帮你评估投产系统的性能
* 来源：
    * 训练集/开发集/测试机最好来源相同，分布相同
    * 可以通过将收集的数据混合，随机打乱后再分配
* 划分：
    * 常规：
        * 数据集较小
        * 7：3——训练集：测试集
        * 6：2：2——训练集：开发集：测试集
    * 现代：
        * 数据集较大
        * 开发集、测试集达到一定的数量即可，占比可以小一点
        * 比如一百万个数据中，1%给开发集，1%给测试集
    * 测试集的省略：
        * 不建议，除非训练集非常大

## 学习过程
* 一般步骤
    * 第一步：定目标，定评价指标
    * 第二步：优化学习路线，提升评分

* 可能出现的问题：
    * 在应用时，客户使用的图像和训练收集的图像清晰度差异过大导致效果差：修改测试集，使其能够更好反映实际需要处理好的数据

* 快速搭建系统，然后进行迭代
    * 快速确定开发集和测试集以及目标错在->搭建一个机器系统原型->通过分析方差、偏差、标记错误等来确定改善方向->改善，迭代训练
    * 一般最开始搭建的模型不用太复杂，简单的模型原型即可，除非有非常相似用途的已经成熟的模型

## 训练上限
* 贝叶斯最优错误率
    * 定义
        * 论上可能达到的最优错误率，就是说没有任何办法设计出一个到的函数，让它能够超过一定的准确度。
    * 理解
        * 比如图片过于模糊/音频过于嘈杂，即使是人也很难判断
    * 表现
        * 训练时达到一定准确率后逐渐训练优化效果减小，难以超过某个上限
    * 一般将人的表现（错误率）作为贝叶斯最优错误率
        * 注意可能出现不同人的表现不同的情况，选择哪个错误率根据实际情况决定
    * 理论上训练效果不会超过贝叶斯最优错误率，除非过拟合

* 可避免错误率
    * 计算
        * 训练集错误率-贝叶斯最优错误率
    * 方差：训练集错误率-测试集错误率
    * 意义
        * 比较可避免错误率和方差，对大的进行改进
            * 可避免错误率较大：从改善模型进行考虑——加深网络/加大训练时间/优化算法/更好的参数/激活函数
            * 方差较大：从避免过拟合进行考虑——收集更多数据/正则化/dropout/数据增强

* 超越人的训练效果
    * 如推荐算法、物流预测
    * 特点
        * 运用了大量的历史数据
        * 不是自然感知问题，计算机处理自然感知问题要超越人更难

## 误差分析
* 分析内容
    * 在开发集/测试集内观察错误标记的样本——假阳性（被错误标记为1的样本）和假阴性（被错误标记为0的样本）
    * 统计不同错误原因占比
    * 分析不同错误原因对整体的正确率的影响（整体错误率*占比），来判断哪些问题需要优先解决

* 例：
    * 人工统计错误被标记为猫的样本中，多少实际上为狗，多少实际上为大型猫科动物，比较哪一个影响更大，然后着重改善这一方面

* 样本本身的标记出错（人工原因）
    * 错误类型
        * 错误样本随机错误：影响不大，不用修正
        * 错误样本体现系统性错误：如果严重影响了判断，则应该花时间修正（如将白色的狗都标记为猫）
    * 修正
        * 修正开发集和测试集
            * 由于开发集和测试集的样本数量较少，一般只对其进行修正，而不是训练集
            * <mark>修正应该同时作用在开发集和测试集中——两者需要来自同一分布，而训练集稍微不同分布影响不大</mark>

## 使用不同分布的数据
* 意义
    * 可能满足实际需求的数据不多（如像客户上传的图像一般模糊度的数据），导致训练困难，就可以增加其他数据（如网上爬取的高清图像）

* 分配方法
    * 方法一：全部混合后分配在训练集、开发集、测试集
        * 优点：所有数据集分布相同，便于管理
        * 缺点：开发集，测试集不是真正想要的数据分布
    * 方法二：将满足精确要求的数据分给开发集和测试集，剩下的满足要求的和不太满足要求的数据给训练集
        * 优点：开发集和测试集正确模拟了需要的数据分布，长期来看会带来更好的系统性能
        * 缺点：出现数据不匹配问题（训练集分布不同）

* 数据不匹配问题
    * 影响判断：训练集和开发集的错误率差异是由于数据不匹配造成还是方差
        * 在训练集中分出一部分作为*训练-开发集*
        * 如果：
            * 训练集和训练开发集错误率差异远大于训练-开发集和开发集，则是由于方差造成
            * 训练-开发集和开发集错误率差异远大于训练集和训练-开发集，则是由于数据不匹配问题造成
        * 其他问题
            * 训练集错误率和贝叶斯错误率差距大，则有可避免偏差问题
            * 测试集错误率和开发集错误率偏差措大->可能对开发集过拟合->需要更大的开发集
            * 如果算法在开发集和测试集上反而表现更好，可能实际需求的数据更好处理、更便于识别
    * 处理
        * 亲自分析，了解训练集和开发、测试集之间的具体差异
        * 收集更多类似开发集和测试集的数据
        * 尝试将训练集变得更像开发/测试集
            * 人工合成数据：如在无噪声音频上合成噪声/使用计算机合成图像<mark>(注意：避免合成的只是一个小子集：如合成使用的噪音都是同一段)</mark>

## 迁移学习
* 定义
    * 使用已经学习好的神经网络模型（*预训练*）来帮助学习新的内容和对象（*微调*）

* 方法
    * 如果新的对象有关数据较少->改变最后一层/几层的权重，重新初始化训练
    * 如果新的对象有关数据较多->改变更多层的权重，重新初始化训练

* 意义
    * 新的内容的有关数据可能不是很多，不便于训练，可以用其他数据进行预训练来帮助学习

* 使用条件
    * 当参与预训练的数据集远大于微调使用的数据集时，即真正需要的数据集不够时

* 原理理解
    * 预训练学习了很多低层次特征，比如边缘检测、曲线检测、结构信息等，从大数据库学习的这些能力可以帮助其他问题的学习处理

## 多任务学习
* 定义
    * 试图让单个神经网络同时做几件事情，然后希望这里每个任务都能帮到其他所有任务。
    * 如同时检测图像中是否有车、路牌等多个物体

* 多任务学习的损失函数
    * ![loss-1](./img/loss-1.png)
    * ![loss-2](./img/loss-2.png)

* 部分标记情况
    * 在计算损失函数时，只考虑标记了的内容，没标记的不加入计算

* 优势
    * 神经网络的训练得到的早期低层次特征各个任务都会用到，可以共用
    * 往往多任务训练比每个任务单独分开效果更好
    * 可以通过其他任务的样本来增加数据集的大小，增强任务性能

* 使用场合
    * 神经网络足够大
    * 每个任务的数据集大小小于其他任务数据集之和的大小（和迁移学习相似）

## 端到端的深度学习
* 定义 
    * 以前有一些数据处理系统或者学习系统，它们需要多个阶段的处理。那么端到端深度学习就是忽略所有这些不同的阶段，用单个神经网络代替它。

* 使用条件
    * 需要足够大的数据才能够表现良好，数据集较小时使用流水线表现效果更好

* 评价
    * 优势
        * 直接学习数据集到结果中间的映射，绕过中间步骤，省去手工设计的组件
        * 只是让数据说话，减少了人为的成见
    * 缺点
        * 排除了可能有用的手工设计组件

* 非端到端的方法
    * 分解了过程，分为了多个步骤
    * 例：门禁人脸识别
        * 现找出人脸的位置：通过一个神经网络单独训练
        * 截出人脸部分，和数据库人脸判断是否是同一个人：通过另一个神经网络单独训练、
    * 分解步骤的优点
        * 每个问题更简单
        * 每个子任务的训练集更多，如人脸位置识别数据、人脸判断较多，而直接不同角度人脸判断数据较少
    * 当数据集足够多时可以变成端到端的学习